---
title: "05 - Analyze: Base"
author: Jeffrey W. Hollister & Emily Read
layout: post_page
---


The focus of this workshop hasn't really been statistics, its been more about R, the language.  But, it is pretty much impossible to talk a lot about R without getting into stats, as that is what draws most people to R in the first place.  So, we will spend a little bit of time on it.  In this lesson we will touch on some very simple stats that we can do with base R as well as provide a quick (i.e. top of my head as I write this) list of what can be done in base R.  

##Quick Links to Exercises and R code
- [Lesson 5 R Code](/introR/rmd_posts/2015-01-15-05-Analyze.R): All the code from this post in an R Script.
- [Exercise 1](#exercise-1): Run some basic statistical test and build a simple model with USGS data

##Lesson Goals
- Conduct basic statistical analyses with base R
- Get a taste of wide array of analyses in base R

##Base statistics
The capabilities that come out of the box with R are actually quite good and used to (i.e. before R) cost you quite a bit to access.  Now it all comes for free!  Some of the things you can do with R without any additional packages inlcude: logistic regression (and all manner of generalized linear models), correlation, principle components analysis, chi-squared tests, clustering, loess, ANOVA, MANOVA, ...  In short, we can do a lot without moving out of base r and `stats`.

We will talk about a few analyses just to show the tip of the iceberg of what is available.

### t-tests
A t-test is done simply with `t.test()` and you control the specifics (paired, two-sided, etc.) with options.  For the simple case of comparing the difference of two means we can use all of the defaults:


{% highlight r %}
x<-rnorm(30,mean=3,sd=2)
y<-rnorm(30,mean=10,sd=5)
xy_tt<-t.test(x,y)
xy_tt
{% endhighlight %}



{% highlight text %}
## 
## 	Welch Two Sample t-test
## 
## data:  x and y
## t = -7.3534, df = 35.58, p-value = 1.218e-08
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -10.172894  -5.773133
## sample estimates:
## mean of x mean of y 
##  3.064824 11.037838
{% endhighlight %}

`t.test()` can also take a formula specification as input. I mentioned this briefly in the previous lesson.  At it's simplest a formula takes the form of `y ~ x`.  The tilde is used in place of the equals sign.  For a t-test that is all we need to know.  


{% highlight r %}
#Lets pick another dataset, ToothGrowth.
#Looking at diff in tooth length by groups specified in supp
rbind(head(ToothGrowth),tail(ToothGrowth))
{% endhighlight %}



{% highlight text %}
##     len supp dose
## 1   4.2   VC  0.5
## 2  11.5   VC  0.5
## 3   7.3   VC  0.5
## 4   5.8   VC  0.5
## 5   6.4   VC  0.5
## 6  10.0   VC  0.5
## 55 24.8   OJ  2.0
## 56 30.9   OJ  2.0
## 57 26.4   OJ  2.0
## 58 27.3   OJ  2.0
## 59 29.4   OJ  2.0
## 60 23.0   OJ  2.0
{% endhighlight %}



{% highlight r %}
t.test(ToothGrowth$len~ToothGrowth$supp)
{% endhighlight %}



{% highlight text %}
## 
## 	Welch Two Sample t-test
## 
## data:  ToothGrowth$len by ToothGrowth$supp
## t = 1.9153, df = 55.309, p-value = 0.06063
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.1710156  7.5710156
## sample estimates:
## mean in group OJ mean in group VC 
##         20.66333         16.96333
{% endhighlight %}
There's a lot more you can do with `t.test()`, but you'll have to rely on `?t.test` for more info.

### Correlation
Next let's take a look at correlations.  Little inspiration is striking, so `iris` it is.


{% highlight r %}
#A simple correlation
cor(iris$Petal.Width,iris$Petal.Length)
{% endhighlight %}



{% highlight text %}
## [1] 0.9628654
{% endhighlight %}



{% highlight r %}
#And a test of that correlation
cor.test(iris$Petal.Width,iris$Petal.Length)
{% endhighlight %}



{% highlight text %}
## 
## 	Pearson's product-moment correlation
## 
## data:  iris$Petal.Width and iris$Petal.Length
## t = 43.3872, df = 148, p-value < 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.9490525 0.9729853
## sample estimates:
##       cor 
## 0.9628654
{% endhighlight %}



{% highlight r %}
#A data frame as input returns a correlation matrix
cor(iris)
{% endhighlight %}



{% highlight text %}
## Error in cor(iris): 'x' must be numeric
{% endhighlight %}



{% highlight r %}
#Oops, non-numeric data.  Lets use dplyr to get what we want and pipe to cor
library(dplyr)
select(iris,-Species) %>% 
  cor()
{% endhighlight %}



{% highlight text %}
##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000
{% endhighlight %}

If you look at the help for `cor()`, you'll see two main optional arguments.  First is the `use` argument which allows you to use the entire dataset or select complete cases which is useful when you have `NA` values.  There are several options.  Also, the default correlation method is for Pearson's.  If you would like to us non-parametric correlations (e.g. rank), you specify that here.

### Linear Regression
Next let's take a look at linear regression.  One of the common ways of accessing linear regressions is with `lm()`.  We have already seen the formula object so there isn't too much that is new here.  Some of the options though are new and useful.  Let's take a look:


{% highlight r %}
lm(Ozone~Temp,data=airquality)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = Ozone ~ Temp, data = airquality)
## 
## Coefficients:
## (Intercept)         Temp  
##    -146.995        2.429
{% endhighlight %}



{% highlight r %}
#Not much info, so save to object and use summary
lm_aq1<-lm(Ozone~Temp,data=airquality)
summary(lm_aq1)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = Ozone ~ Temp, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***
## Temp           2.4287     0.2331  10.418  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877,	Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16
{% endhighlight %}



{% highlight r %}
#And now a multiple linear regression
lm_aq2<-lm(Ozone~Temp+Wind+Solar.R,data=airquality)
summary(lm_aq2)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = Ozone ~ Temp + Wind + Solar.R, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.485 -14.219  -3.551  10.097  95.619 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -64.34208   23.05472  -2.791  0.00623 ** 
## Temp          1.65209    0.25353   6.516 2.42e-09 ***
## Wind         -3.33359    0.65441  -5.094 1.52e-06 ***
## Solar.R       0.05982    0.02319   2.580  0.01124 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 21.18 on 107 degrees of freedom
##   (42 observations deleted due to missingness)
## Multiple R-squared:  0.6059,	Adjusted R-squared:  0.5948 
## F-statistic: 54.83 on 3 and 107 DF,  p-value: < 2.2e-16
{% endhighlight %}

All of your standard modelling approaches (and then some) are available in R including typical variable selection techniques (e.g. stepwise with AIC) and logistic regression, which is implemented with the rest of the generalized linear models in `glm()`.  Interaction terms can be specified directly in the model, but there are several ways to do so (x*y, x:y, x^y).  Lastly, if you are interested in more involved or newer approaches these will be implemented (most likely) with additional packages and will not be available in base R or `stats`.



##Excercise 1
For this exercise, let's start to look at some of the statistical tests and relationships for NWIS data we collected for GAGES-II sites in NM. We have already plotted some of these variables together, so you may have an idea ahead of time about what the statistical tests might show. 

1. First, lets take a look at the relationship between discharge, temperature specific conductance, and phosphorus concentration across Sentinel and non-Sentinel sites.  Add a section to your script that tests for a difference in the mean value of across these between NSIP Sentinal sites and non-Sentinel sites. For this we will have the script save each test to a new object and print the results to the screen. Hint: consider that there are many fewer Sentinel sites than non-Sentinel sites. Consider the statistical implcations for the differences in sample sizes. 

2. Next, lets build some linear models that build a linear model to be used to predict phosphorus concentrations across all GAGES-II sites in NM. Chose at least three, and as many as ten, explanatory variables that you think may be useful to predicting phosphorus. Again save your model to an object and print the summary of that model to the screen. 
